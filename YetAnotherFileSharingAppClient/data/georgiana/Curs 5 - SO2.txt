Curs 5 - SO2ss

Desi, echipamentele pot fi controlate in intregime prin intermediul porturilor I/O sau zone de memorie specializate.
Interrupt ReQuest oposed to polling, unde CPU-l intreaba mereu hardware-ul de starea lor..., polling nu e chiar ok...

Atunci când trebuie să accesăm resurse partajate între o rutină de tratare a întreruperii (A) și cod ce rulează în context proces sau într-o rutină de tratare a unei acțiuni amânabile (B), trebuie să folosim un mod special de sincronizare. În (A) trebuie să folosim o primitivă de tip spinlock, iar în (B) trebuie să dezactivăm întreruperile ȘI să folosim o primitivă de tip spinlock.

bts - face bit test and set, o intrebare la curs era de ce bts face write si daca valoarea lock-ului e deja 1..., practic invalideaza mereu o linie de cache ... ?? Pai ideea e ca ai nevoie de operatii atomice sa iei un lock, nu poti sa testezi si dupa sa set-ezi aia pe unu, ca poate intre timp testeaza unu' si ii da tot 0, si practic intrati amandoi in aceeasi regiune critica ....

Daca ai mai multe lock-uri nested, cum ar fi cat, dog si fox, si o functie le ia in aceasta ordine ..., atunci orice alta functie ar trebui sa le ia tot in aceasta ordine ... asa se previn deadlock-urile, e good practice sa spui in comentarii ordinea.

Contention of a lock: High contention can occour because a lock is frequently obtained, held for a long time after it is obtained, or both.

Finer grained lock - protects a samll amaount a data, say only a single element in a larger structure.
A coarse lock - protects a large amaount of data - for example an entire sub-system's set of data structures.
Exemplue de evolving finer-grained locking is the scheduler ____runqueues_____. In <= 2.4 kernel exista un singur runqueue pentru toate procesoarele, acum in >= 2.6, s cheduler O(1) a introdus cate un ____runqueue___ pentru fiecare procesor, each with a unique lock.
RUNQUEUE: List of runnable processes...p
This was an important optimization, because the runqueue lock was highly contended on large machines, essentially serializing the entire scheduling process down to a single processor executing in the scheduler at a time. Dupa, tot in 2.6 the """""""""CFS SCheduler"""""""""" improved scalability firther.

The atomic operations are typically implemented as inline functions with inline assembly.
In the case where a specific function is inherently atomic, the given function is
usually just a macro. For example, on most architectures, a word-sized read is always
atomic.That is, a read of a single word cannot complete in the middle of a write to that
word.The read always returns the word in a consistent state, either before or after the
write completes, but never in the middle. Consequently, atomic_read() is usually just a
macro returning the integer value of the atomic_t, functia contine pur si simplu return v->counter; unde v e primit ca parametru atomic_t *v, si functia e inline

Atomicity Versus Ordering
The preceding discussion on atomic reading begs a discussion on the differences between
atomicity and ordering. As discussed, a word-sized read always occurs atomically. It never interleaves
with a write to the same word; the read always returns the word in a consistent
state—perhaps before the write completes, perhaps after, but never during. For example, if
an integer is initially 42 and then set to 365, a read on the integer always returns 42 or 365
and never some commingling of the two values. We call this atomicity.
Your code, however, might have more stringent requirements than this: Perhaps you require
that the read always occurs before the pending write. This type of requirement is not atomicity,
but ordering. Atomicity ensures that instructions occur without interruption and that they
complete either in their entirety or not at all. Ordering, on the other hand, ensures that the
desired, relative ordering of two or more instructions—even if they are to occur in separate
threads of execution or even separate processors—is preserved.
The atomic operations discussed in this section guarantee only atomicity. Ordering is enforced
via barrier operations, which we discuss later in this chapter.

Planifici chestii in bottom half pentru ca in intrerupere ar dura prea mult..

As discussed in Chapter 8,“Bottom Halves and Deferring Work,” certain locking precautions
must be taken when working with bottom halves.The function spin_lock_bh()
obtains the given lock and disables all bottom halves.The function spin_unlock_bh()
performs the inverse.
Because a bottom half might preempt process context code, if data is shared between a
bottom-half process context, you must protect the data in process context with both a
lock and the disabling of bottom halves. Likewise, because an interrupt handler might
preempt a bottom half, if data is shared between an interrupt handler and a bottom half,
you must both obtain the appropriate lock and disable interrupts.
Recall that two tasklets of the same type do not ever run simultaneously.Thus, there is
no need to protect data used only within a single type of tasklet. If the data is shared between
two different tasklets, however, you must obtain a normal spin lock before accessing
the data in the bottom half.You do not need to disable bottom halves because a
tasklet never preempts another running tasklet on the same processor.
With softirqs, regardless of whether it is the same softirq type, if data is shared by
softirqs, it must be protected with a lock. Recall that softirqs, even two of the same type,
might run simultaneously on multiple processors in the system.A softirq never preempts
another softirq running on the same processor, however, so disabling bottom halves is
not needed.

sequential locks, e un fel de rw_lock dar """"""""""""favorizeaza scriitori"""""""""""""", la rw_lock, un writer asteapta sa termine toti cititorii.
Un mare utlizator al seq_locks e jiffies(), jifiies fiind o variabila de 64biti, pe sistemele care nu pot citi 64 de biti in mod atomic, adica de exemplu pe cele de 32 de biti, se foloseste get_jifiies_64() care e implementata folosinf sequential locks

if a spin lock is held, the kernel is not preemptive. Because the concurrency issues with kernel preemption and SMP are
the same, and the kernel is already SMP-safe; this simple change makes the kernel preempt-
safe, too.
The preemption count stores the number of held locks and preempt_disable() calls.
If the number is zero, the kernel is preemptive. If the value is one or greater, the kernel is
not preemptive.
aaaaaa